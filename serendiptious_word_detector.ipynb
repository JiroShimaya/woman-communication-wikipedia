{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリのインストール\n",
    "必要なライブラリをインストールします。後で使うものもまとめてimportします。\n",
    "\n",
    "```\n",
    "pip install pandas datasets mecab-pytyon3 unidic-lite\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリのインストール\n",
    "必要なライブラリをimportします。後で使うものもまとめてimportします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import MeCab\n",
    "import copy\n",
    "import tqdm\n",
    "from typing import Dict, List, Any, Tuple, Optional, Callable\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解析用クラスの作成\n",
    "文章を発音に変換する関数や単語境界をまたぐ有無などを判定する関数などをひとまとめにしたクラスを作ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SerendipitousWordDetector:\n",
    "    def __init__(self, tokenize_func: Optional[Callable[[str], Tuple[List[str], List[str]]]] = None):\n",
    "        \"\"\"\n",
    "        SerendipitousWordDetectorクラスのコンストラクタです。\n",
    "        形態素解析を行うためのトークナイズ関数を受け取り、インスタンス変数に設定します。\n",
    "        もしトークナイズ関数が指定されていない場合は、デフォルトのトークナイズ関数を使用します。\n",
    "\n",
    "        Args:\n",
    "            tokenize_func (Optional[Callable[[str], Tuple[List[str], List[str]]]], optional): \n",
    "                形態素解析を行うための関数。\n",
    "                この関数はテキストを引数に取り、表層形と発音のトークンのリストをタプルで返す必要があります。\n",
    "                もしNoneが指定された場合は、MeCabを使用したデフォルトのトークナイズ関数が使用されます。\n",
    "        \"\"\"\n",
    "        if callable(tokenize_func):\n",
    "            self.tokenize = tokenize_func\n",
    "        else:\n",
    "            self.tokenize = self.get_default_tokenize_func()\n",
    "                            \n",
    "    def get_pronunciation(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        与えられたテキストを形態素解析し、その発音を連結した文字列を返します。\n",
    "\n",
    "        Args:\n",
    "            text (str): 発音を取得したい日本語のテキスト。\n",
    "\n",
    "        Returns:\n",
    "            str: テキストの発音を表す文字列。\n",
    "        \"\"\"\n",
    "        _, pronunciation_tokens = self.tokenize(text)\n",
    "        return \"\".join(pronunciation_tokens)    \n",
    "    def get_default_tokenize_func(self):\n",
    "\n",
    "        mecab = MeCab.Tagger()\n",
    "\n",
    "        def _tokenize(text: str) -> Tuple[List[str], List[str]]:\n",
    "            \"\"\"\n",
    "            与えられたテキストを形態素解析し、表層形と発音のトークンのリストを返します。\n",
    "\n",
    "            Args:\n",
    "                text (str): 形態素解析を行いたい日本語のテキスト。\n",
    "\n",
    "            Returns:\n",
    "                Tuple[List[str], List[str]]: \n",
    "                    - 最初のリストはテキストの表層形のトークンを含みます。\n",
    "                    - 二番目のリストは対応する発音のトークンを含みます。\n",
    "                    これらのリストは同じ長さで、各表層形のトークンは対応する発音のトークンと位置を合わせています。\n",
    "            \"\"\"\n",
    "            lines = mecab.parse(text).splitlines()[:-1]  # EOSを除外するために最後の行を除く\n",
    "            surface_tokens = [line.split(\"\\t\")[0] for line in lines]  # 各行の表層形を抽出\n",
    "            pronunciation_tokens = [line.split(\"\\t\")[1] for line in lines]  # 各行の発音を抽出\n",
    "\n",
    "            return surface_tokens, pronunciation_tokens\n",
    "\n",
    "        return _tokenize\n",
    "    \n",
    "    def is_word_used_in_original_context(\n",
    "            self,\n",
    "            word_surface: str,\n",
    "            word_pronunciation: str,\n",
    "            passage_surface: str,\n",
    "            passage_pronunciation_tokens: List[str]\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        単語が元の文脈（意味）で使用されているかどうかを判定します。\n",
    "\n",
    "        この関数は、単語の表層形が文章中に存在するか、または単語の発音が文章の発音トークンリストに含まれているかどうかをチェックします。\n",
    "        どちらか一方が真であれば、その単語は元の文脈で使用されていると判定されます。\n",
    "\n",
    "        Args:\n",
    "            word_surface (str): チェックしたい単語の表層形。\n",
    "            word_pronunciation (str): チェックしたい単語の発音。\n",
    "            passage_surface (str): チェック対象の文章。\n",
    "            passage_pronunciation_tokens (List[str]): 文章の発音をトークン化したリスト。\n",
    "\n",
    "        Returns:\n",
    "            bool: 単語が元の文脈で使用されている場合はTrue、そうでない場合はFalse。\n",
    "        \"\"\"\n",
    "        return (\n",
    "            word_surface in passage_surface or\n",
    "            word_pronunciation in passage_pronunciation_tokens\n",
    "        )    \n",
    "\n",
    "    def is_crossing_word_boundary(\n",
    "            self,\n",
    "            word_surface: str,\n",
    "            word_pronunciation: str,\n",
    "            passage_surface: str,\n",
    "            passage_pronunciation_tokens: List[str]\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        単語が文章の単語境界をまたいでいるかどうかを判定します。\n",
    "\n",
    "        この関数は、単語の発音が文章の発音トークン列に含まれているか（contains_word）、\n",
    "        単語が元の文脈で使用されていないか（is_word_used_in_original_context）、\n",
    "        そして単語の発音が単語境界をまたがない形で文章の発音トークン列に含まれているか（is_not_crossing_word_boundary）\n",
    "        をチェックします。\n",
    "\n",
    "        Args:\n",
    "            word_surface (str): チェックしたい単語の表層形。\n",
    "            word_pronunciation (str): チェックしたい単語の発音。\n",
    "            passage_surface (str): チェック対象の文章の表層形。\n",
    "            passage_pronunciation_tokens (List[str]): 文章の発音をトークン化したリスト。\n",
    "\n",
    "        Returns:\n",
    "            bool: 単語が単語境界をまたいでいる場合はTrue、そうでない場合はFalse。\n",
    "        \"\"\"\n",
    "        # 単語の発音が文章の発音トークン列に含まれているかどうか\n",
    "        contains_word = word_pronunciation in \"\".join(passage_pronunciation_tokens)\n",
    "        \n",
    "        # 単語が元の文脈で使用されているかどうか\n",
    "        is_word_used_in_original_context = self.is_word_used_in_original_context(\n",
    "            word_surface, word_pronunciation, passage_surface, passage_pronunciation_tokens\n",
    "        )\n",
    "        \n",
    "        # 単語の発音が単語境界をまたがない形で文章の発音トークン列に含まれているかどうか\n",
    "        is_not_crossing_word_boundary = word_pronunciation in \" \".join(passage_pronunciation_tokens)\n",
    "\n",
    "        # すべての条件を満たす場合、単語は単語境界をまたいでいると判定\n",
    "        return contains_word and not is_word_used_in_original_context and not is_not_crossing_word_boundary\n",
    "    \n",
    "    def get_word_context(\n",
    "            self,\n",
    "            word_pronunciation: str,\n",
    "            passage_surface_tokens: List[str],\n",
    "            passage_pronunciation_tokens: List[str],\n",
    "            context_range: Tuple[int] = [10, 10]\n",
    "    )->str:\n",
    "        \"\"\"\n",
    "        指定された単語の発音に基づいて、その単語が含まれる文章の一部分を抽出します。\n",
    "        文章は表層形のトークンのリストと発音のトークンのリストで表され、\n",
    "        単語の発音が文章中で最初に現れる位置を基準に前後のトークンを含めた範囲を返します。\n",
    "\n",
    "        Args:\n",
    "            word_pronunciation (str): 抽出したい単語の発音。\n",
    "            passage_surface_tokens (List[str]): 文章の表層形のトークンのリスト。\n",
    "            passage_pronunciation_tokens (List[str]): 文章の発音のトークンのリスト。\n",
    "            context_range (int, optional): 抽出する範囲のトークン数。デフォルトは20。\n",
    "\n",
    "        Returns:\n",
    "            str: 指定された単語を含む文章の一部分。\n",
    "        \"\"\"\n",
    "\n",
    "        # 単語の発音が文章の発音トークン列で最初に現れる位置を見つける\n",
    "        pronunciation_pos = \"\".join(passage_pronunciation_tokens).index(word_pronunciation)\n",
    "\n",
    "        # 単語が見つかったトークンのインデックスを特定する\n",
    "        hit_index_start, hit_index_end = -1, -1\n",
    "        current_pos = 0\n",
    "        for i, p in enumerate(passage_pronunciation_tokens):\n",
    "            current_pos += len(p)\n",
    "            # 現在のトークンの終わりが単語の発音の開始位置を超えたらループを終了\n",
    "            if current_pos > pronunciation_pos and hit_index_start < 0:\n",
    "                hit_index_start = i\n",
    "            if current_pos >= pronunciation_pos + len(word_pronunciation):\n",
    "                hit_index_end = i + 1\n",
    "                break\n",
    "\n",
    "        # 抽出する範囲の開始インデックスを計算する\n",
    "        start_index = max(0, hit_index_start-context_range[0])\n",
    "        # 抽出する範囲の終了インデックスを計算する\n",
    "        end_index = min(hit_index_end+context_range[1], len(passage_surface_tokens))\n",
    "\n",
    "        # 指定された範囲の表層形トークンを結合して返す\n",
    "        return \"\".join(passage_surface_tokens[start_index: end_index])\n",
    "    \n",
    "    def check_hit(self, word_surface: str, word_pronunciation: str\n",
    "                  , passage_surface_tokens: List[str], passage_pronunciation_tokens: List[str]\n",
    "                  )->Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        単語が文章中にヒットするかどうかをチェックし、ヒットした場合にはその単語の情報を含む辞書を返します。\n",
    "\n",
    "        Args:\n",
    "            word_surface (str): チェックしたい単語の表層形。\n",
    "            word_pronunciation (str): チェックしたい単語の発音。\n",
    "            passage_surface_tokens (List[str]): 文章の表層形のトークンのリスト。\n",
    "            passage_pronunciation_tokens (List[str]): 文章の発音のトークンのリスト。\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: 単語のヒット情報を含む辞書。ヒットしたかどうか、単語の表層形、発音、長さ、\n",
    "                             単語境界をまたいでいるかどうか、コンテキスト、マッチした文章の表層形を含む。\n",
    "        \"\"\"\n",
    "        # 単語が文章の発音トークン列に含まれているかどうか、および元のコンテキストで使用されていないかどうかをチェック\n",
    "        hit = word_pronunciation in \"\".join(passage_pronunciation_tokens) and not self.is_word_used_in_original_context(word_surface, word_pronunciation, \"\".join(passage_surface_tokens), passage_pronunciation_tokens)\n",
    "        # 結果を格納する辞書を初期化\n",
    "        result = {\n",
    "            \"hit\": hit,\n",
    "            \"surface\": word_surface,\n",
    "            \"pronunciation\": word_pronunciation,\n",
    "            \"length\": len(word_pronunciation)\n",
    "        }\n",
    "        # ヒットした場合、追加の情報を辞書に追加\n",
    "        if hit:\n",
    "            # 単語が単語境界をまたいでいるかどうかをチェック\n",
    "            is_crossing_boundary = self.is_crossing_word_boundary(word_surface, word_pronunciation, \"\".join(passage_pronunciation_tokens), passage_pronunciation_tokens)\n",
    "            # 単語のコンテキストを取得\n",
    "            context = self.get_word_context(word_pronunciation, passage_surface_tokens, passage_pronunciation_tokens)\n",
    "            if is_crossing_boundary:\n",
    "                matched_passage_surface = self.get_word_context(word_pronunciation, passage_surface_tokens, passage_pronunciation_tokens, [0,0])\n",
    "            else:\n",
    "                matched_passage_surface = self.get_word_context(word_pronunciation, passage_surface_tokens, passage_pronunciation_tokens, [0,0])\n",
    "            result.update({\n",
    "                \"is_crossing_word_boundary\": is_crossing_boundary,\n",
    "                \"context\": context,\n",
    "                \"matched_passage_surface\": matched_passage_surface\n",
    "            })\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "        \n",
    "    def check_hits(self, word_df: pd.DataFrame, passage_df: pd.DataFrame\n",
    "                   )->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        単語リストと文章リストを受け取り、各文章に含まれる単語のヒット情報をチェックします。\n",
    "\n",
    "        Args:\n",
    "            word_df (pd.DataFrame): チェックしたい単語のデータフレーム。'surface'と'pronunciation'の列が必要です。\n",
    "            passage_df (pd.DataFrame): チェック対象の文章のデータフレーム。'surface'列が必要です。\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 各文章にヒットした単語の情報を含む新しいデータフレーム。\n",
    "        \"\"\"\n",
    "        # データフレームのディープコピーを作成\n",
    "        word_df = copy.deepcopy(word_df)\n",
    "        passage_df = copy.deepcopy(passage_df)\n",
    "\n",
    "        # 単語データフレームに発音列がない場合は生成\n",
    "        if \"pronunciation\" not in word_df.columns:\n",
    "            word_df[\"pronunciation\"] = word_df[\"surface\"].map(self.get_pronunciation)\n",
    "        # 文章データフレームに発音トークン列がない場合は生成\n",
    "        if \"pronunciation_tokens\" not in passage_df.columns:\n",
    "            passage_df[\"surface_tokens\"] = passage_df[\"surface\"].map(lambda x: self.tokenize(x)[0])\n",
    "            passage_df[\"pronunciation_tokens\"] = passage_df[\"surface\"].map(lambda x: self.tokenize(x)[1])\n",
    "            passage_df[\"pronunciation\"] = passage_df[\"pronunciation_tokens\"].map(lambda x: \"\".join(x))\n",
    "\n",
    "        # ヒットした単語を格納するリストを初期化\n",
    "        hitwords_list = []\n",
    "        # 各文章に対してヒットした単語をチェック\n",
    "        for passage_surface_tokens, passage_pronunciation_tokens in tqdm.tqdm(zip(passage_df[\"surface_tokens\"], passage_df[\"pronunciation_tokens\"])):\n",
    "            hitwords = []\n",
    "            # 各単語に対してヒットチェックを行い、ヒットしたものをリストに追加\n",
    "            for word_surface, word_pronunciation in zip(word_df[\"surface\"], word_df[\"pronunciation\"]):\n",
    "                result = self.check_hit(word_surface, word_pronunciation\n",
    "                                   , passage_surface_tokens, passage_pronunciation_tokens)\n",
    "                if result[\"hit\"]:\n",
    "                    hitwords.append(result)\n",
    "            \n",
    "            # ヒットした単語の情報を含むオブジェクトを作成\n",
    "            hitwords_obj = {\"hitwords\": hitwords, \"hit\": False}\n",
    "            if hitwords:\n",
    "                hitwords_obj.update({\n",
    "                    \"hit\": True\n",
    "                    , \"sum_length\": sum([obj[\"length\"] for obj in hitwords])\n",
    "                    , \"max_length\": max([obj[\"length\"] for obj in hitwords])\n",
    "                    , \"num\": len(hitwords)\n",
    "                })\n",
    "            hitwords_list.append(hitwords_obj)\n",
    "        \n",
    "        # ヒット情報を文章データフレームに追加\n",
    "        passage_df[\"hitword\"] = hitwords_list\n",
    "        # ヒットした単語がある文章のみをフィルタリング\n",
    "        passage_df = passage_df[passage_df[\"hitword\"].map(lambda x: x[\"hit\"])]\n",
    "        return passage_df\n",
    "\n",
    "    def convert_table(self, passage_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        passage_dfから新しいデータフレームを作成し、各行にヒットした単語の情報を含む表を生成します。\n",
    "\n",
    "        Args:\n",
    "            passage_df (pd.DataFrame): 'curid', 'title', および 'hitword' の列を含むデータフレーム。\n",
    "                'hitword' 列は、ヒットした単語の情報を含む辞書のリストを持つ必要があります。\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: ヒットした単語の情報を含む新しいデータフレーム。\n",
    "        \"\"\"\n",
    "        new_rows = []\n",
    "        for idx, row in tqdm.tqdm(passage_df.iterrows()):\n",
    "            curid, title = row[\"curid\"], row[\"title\"]\n",
    "            hitwords_obj = row[\"hitword\"]\n",
    "            hitwords = hitwords_obj[\"hitwords\"]\n",
    "            sum_length = hitwords_obj[\"sum_length\"]\n",
    "            max_length = hitwords_obj[\"max_length\"]\n",
    "            num = hitwords_obj[\"num\"]\n",
    "            for hitword in hitwords:\n",
    "                word_surface, word_pronunciation = hitword[\"surface\"], hitword[\"pronunciation\"]\n",
    "                word_length = hitword[\"length\"]\n",
    "                is_crossing_word_boundary = hitword[\"is_crossing_word_boundary\"]\n",
    "                context = hitword[\"context\"]\n",
    "                matched_passage_surface = hitword[\"matched_passage_surface\"]\n",
    "\n",
    "                new_rows.append({\n",
    "                    \"curid\": curid,\n",
    "                    \"title\": title,\n",
    "                    \"word_surface\": word_surface,\n",
    "                    \"word_pronunciation\": word_pronunciation,\n",
    "                    \"word_length\": word_length,\n",
    "                    \"is_crossing_word_boundary\": is_crossing_word_boundary,\n",
    "                    \"context\": context,\n",
    "                    \"matched_passage_surface\": matched_passage_surface,\n",
    "                    \"sum_length\": sum_length,\n",
    "                    \"max_length\": max_length,\n",
    "                    \"num\": num\n",
    "                })\n",
    "        # 新しいデータフレームを作成\n",
    "        df = pd.DataFrame(new_rows)\n",
    "        # 重複する行を削除\n",
    "        df = df.drop_duplicates(subset=['word_surface', 'matched_passage_surface'], keep='first')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テスト\n",
    "各メソッドの出力を確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizeメソッドの出力:\n",
      "(['これ', 'は', 'テスト', 'の', '文章', 'です', '。', '形態', '素', '解析', 'を', '行い', 'ます', '。', 'Python', 'は', 'プログラミング', '言語', 'の', '一', 'つ', 'です', '。'], ['コレ', 'ワ', 'テスト', 'ノ', 'ブンショー', 'デス', '', 'ケータイ', 'ソ', 'カイセキ', 'オ', 'オコナイ', 'マス', '', 'Python', 'ワ', 'プログラミング', 'ゲンゴ', 'ノ', 'ヒト', 'ツ', 'デス', ''])\n",
      "\n",
      "get_pronunciationメソッドの出力:\n",
      "コレワテストノブンショーデスケータイソカイセキオオコナイマスPythonワプログラミングゲンゴノヒトツデス\n",
      "\n",
      "is_word_used_in_original_contextメソッドの出力:\n",
      "単語 'テスト' が元の文脈で使用されているか: True\n",
      "単語 '解析' が元の文脈で使用されているか: True\n",
      "単語 'Python' が元の文脈で使用されているか: True\n",
      "単語 'グゲ' が元の文脈で使用されているか: False\n",
      "\n",
      "is_crossing_word_boundaryメソッドの出力:\n",
      "単語 'テスト' が単語境界をまたいでいるか: False\n",
      "単語 '解析' が単語境界をまたいでいるか: False\n",
      "単語 'Python' が単語境界をまたいでいるか: False\n",
      "単語 'グゲ' が単語境界をまたいでいるか: True\n",
      "\n",
      "get_word_contextメソッドの出力:\n",
      "単語 'テスト' のコンテキスト: これはテストの文章です。形態素解析を行います\n",
      "単語 '解析' のコンテキスト: これはテストの文章です。形態素解析を行います。Pythonはプログラミング言語の一\n",
      "単語 'Python' のコンテキスト: 文章です。形態素解析を行います。Pythonはプログラミング言語の一つです。\n",
      "単語 'グゲ' のコンテキスト: 。形態素解析を行います。Pythonはプログラミング言語の一つです。\n",
      "\n",
      "check_hitメソッドの出力:\n",
      "単語 'テスト' のヒット情報: {'hit': False, 'surface': 'テスト', 'pronunciation': 'テスト', 'length': 3}\n",
      "単語 '解析' のヒット情報: {'hit': False, 'surface': '解析', 'pronunciation': 'カイセキ', 'length': 4}\n",
      "単語 'Python' のヒット情報: {'hit': False, 'surface': 'Python', 'pronunciation': 'Python', 'length': 6}\n",
      "単語 'グゲ' のヒット情報: {'hit': True, 'surface': 'グゲ', 'pronunciation': 'グゲ', 'length': 2, 'is_crossing_word_boundary': False, 'context': '。形態素解析を行います。Pythonはプログラミング言語の一つです。', 'matched_passage_surface': 'プログラミング言語'}\n",
      "\n",
      "check_hitsメソッドの出力:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 3908.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         surface  \\\n",
      "0  これはテストの文章です。形態素解析を行います。Pythonはプログラミング言語の一つです。   \n",
      "\n",
      "                                      surface_tokens  \\\n",
      "0  [これ, は, テスト, の, 文章, です, 。, 形態, 素, 解析, を, 行い, ま...   \n",
      "\n",
      "                                pronunciation_tokens  \\\n",
      "0  [コレ, ワ, テスト, ノ, ブンショー, デス, , ケータイ, ソ, カイセキ, オ,...   \n",
      "\n",
      "                                       pronunciation  \\\n",
      "0  コレワテストノブンショーデスケータイソカイセキオオコナイマスPythonワプログラミングゲン...   \n",
      "\n",
      "                                             hitword  \n",
      "0  {'hitwords': [{'hit': True, 'surface': 'グゲ', '...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SerendipitousWordDetectorクラスの定義は省略（上記で提供されたクラス定義を使用）\n",
    "\n",
    "# テスト用のテキストと単語リストを作成\n",
    "test_text = \"これはテストの文章です。形態素解析を行います。Pythonはプログラミング言語の一つです。\"\n",
    "test_words = ['テスト', '解析', 'Python', 'グゲ']\n",
    "\n",
    "# SerendipitousWordDetectorクラスのインスタンスを作成\n",
    "detector = SerendipitousWordDetector()\n",
    "\n",
    "# tokenizeメソッドのテスト\n",
    "print(\"tokenizeメソッドの出力:\")\n",
    "print(detector.tokenize(test_text))\n",
    "\n",
    "# get_pronunciationメソッドのテスト\n",
    "print(\"\\nget_pronunciationメソッドの出力:\")\n",
    "print(detector.get_pronunciation(test_text))\n",
    "\n",
    "# is_word_used_in_original_contextメソッドのテスト\n",
    "print(\"\\nis_word_used_in_original_contextメソッドの出力:\")\n",
    "surface_tokens, pronunciation_tokens = detector.tokenize(test_text)\n",
    "for word in test_words:\n",
    "    print(f\"単語 '{word}' が元の文脈で使用されているか: {detector.is_word_used_in_original_context(word, detector.get_pronunciation(word), test_text, pronunciation_tokens)}\")\n",
    "\n",
    "# is_crossing_word_boundaryメソッドのテスト\n",
    "print(\"\\nis_crossing_word_boundaryメソッドの出力:\")\n",
    "for word in test_words:\n",
    "    print(f\"単語 '{word}' が単語境界をまたいでいるか: {detector.is_crossing_word_boundary(word, detector.get_pronunciation(word), test_text, pronunciation_tokens)}\")\n",
    "\n",
    "# get_word_contextメソッドのテスト\n",
    "print(\"\\nget_word_contextメソッドの出力:\")\n",
    "for word in test_words:\n",
    "    print(f\"単語 '{word}' のコンテキスト: {detector.get_word_context(detector.get_pronunciation(word), surface_tokens, pronunciation_tokens)}\")\n",
    "\n",
    "# check_hitメソッドのテスト\n",
    "print(\"\\ncheck_hitメソッドの出力:\")\n",
    "for word in test_words:\n",
    "    result = detector.check_hit(word, detector.get_pronunciation(word), surface_tokens, pronunciation_tokens)\n",
    "    print(f\"単語 '{word}' のヒット情報: {result}\")\n",
    "\n",
    "# check_hitsメソッドのテスト\n",
    "print(\"\\ncheck_hitsメソッドの出力:\")\n",
    "word_df = pd.DataFrame({'surface': test_words})\n",
    "passage_df = pd.DataFrame({'surface': [test_text]})\n",
    "hitwords_df = detector.check_hits(word_df, passage_df)\n",
    "print(hitwords_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 3300.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   curid     title word_surface word_pronunciation  word_length  \\\n",
      "0      1  テストタイトル1          テスト                テスト            3   \n",
      "1      1  テストタイトル1           解析               カイセキ            4   \n",
      "2      2  テストタイトル2       Python               パイソン            5   \n",
      "\n",
      "   is_crossing_word_boundary                 context matched_passage_surface  \\\n",
      "0                      False            これはテストの文章です。                     テスト   \n",
      "1                       True             形態素解析を行います。                      解析   \n",
      "2                      False  Pythonはプログラミング言語の一つです。                  Python   \n",
      "\n",
      "   sum_length  max_length  num  \n",
      "0           7           4    2  \n",
      "1           7           4    2  \n",
      "2           5           5    1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# テストデータの作成\n",
    "test_data = {\n",
    "    'curid': [1, 2],\n",
    "    'title': ['テストタイトル1', 'テストタイトル2'],\n",
    "    'hitword': [\n",
    "        {\n",
    "            \"hitwords\": [\n",
    "                {'hit': True, 'surface': 'テスト', 'pronunciation': 'テスト', 'length': 3, 'is_crossing_word_boundary': False, 'context': 'これはテストの文章です。', 'matched_passage_surface': 'テスト'},\n",
    "                {'hit': True, 'surface': '解析', 'pronunciation': 'カイセキ', 'length': 4, 'is_crossing_word_boundary': True, 'context': '形態素解析を行います。', 'matched_passage_surface': '解析'}\n",
    "            ],\n",
    "            \"sum_length\": 7,\n",
    "            \"max_length\": 4,\n",
    "            \"num\": 2\n",
    "        },\n",
    "        {\n",
    "            \"hitwords\": [\n",
    "                {'hit': True, 'surface': 'Python', 'pronunciation': 'パイソン', 'length': 5, 'is_crossing_word_boundary': False, 'context': 'Pythonはプログラミング言語の一つです。', 'matched_passage_surface': 'Python'}\n",
    "            ],\n",
    "            \"sum_length\": 5,\n",
    "            \"max_length\": 5,\n",
    "            \"num\": 1\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# データフレームの作成\n",
    "passage_df = pd.DataFrame(test_data)\n",
    "\n",
    "# SerendipitousWordDetectorクラスのインスタンスを作成\n",
    "detector = SerendipitousWordDetector()\n",
    "\n",
    "# convert_tableメソッドをテスト\n",
    "converted_df = detector.convert_table(passage_df)\n",
    "\n",
    "# 結果の表示\n",
    "print(converted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 単語リストのダウンロードと整形\n",
    "\n",
    "単語リストをダウンロードし、data/poke.csvというパスに保存します。\n",
    "\n",
    "```sh\n",
    "# macの場合\n",
    "curl -o data/poke.txt https://wonderhorn.net/material/poke.txt\n",
    "```\n",
    "\n",
    "poke.csvの中身は以下のような感じです。shiftjisでencodeされていることに注意してください。\n",
    "\n",
    "```data/poke.txt\n",
    "フシギダネ\n",
    "フシギソウ\n",
    "フシギバナ\n",
    "ヒトカゲ\n",
    "...\n",
    "```\n",
    "\n",
    "surfaceという列名をもつcsvとして、utf8で保存し直します。もしくは手動で書き換え、再エンコードを行っても良いと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_poketext(input_path: str, output_path: str\n",
    "                    , *\n",
    "                    , input_encoding = \"shiftjis\"\n",
    "                    , output_encoding = \"utf8\")->None:\n",
    "    df = pd.read_csv(input_path, header=None, encoding=input_encoding)\n",
    "    df.columns=[\"surface\"]\n",
    "    df.to_csv(output_path, encoding=output_encoding, index=False)\n",
    "  \n",
    "RAW_FILE_PATH = \"data/poke.txt\"\n",
    "WORDLIST_PATH = \"data/poke_formatted.csv\"\n",
    "\n",
    "format_poketext(RAW_FILE_PATH, WORDLIST_PATH)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['curid', 'title', 'text'],\n",
      "        num_rows: 1362415\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# URL of the dataset\n",
    "dataset_url = \"izumi-lab/wikipedia-ja-20230720\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_url)\n",
    "\n",
    "# To view basic information or manipulate the dataset, you can use:\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  curid   title                                            surface\n",
      "0     5  アンパサンド  アンパサンド（&amp;, ）は、並立助詞「…と…」を意味する記号である。ラテン語で「…と…...\n",
      "1    10      言語  言語（げんご、language）は、狭義には「声による記号の体系」をいう。\\n広辞苑や大辞泉...\n",
      "2    11     日本語  日本語（にほんご、にっぽんご、）は、日本国内や、かつての日本領だった国、そして国外移民や移住...\n"
     ]
    }
   ],
   "source": [
    "subset = dataset[\"train\"][:3]\n",
    "passage_df = pd.DataFrame(subset)\n",
    "passage_df.rename(columns={\"text\": \"surface\"}, inplace=True)\n",
    "print(passage_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "  curid   title                                            surface\n",
      "0     5  アンパサンド  アンパサンド（&amp;, ）は、並立助詞「…と…」を意味する記号である。ラテン語で「…と…...\n",
      "1    10      言語  言語（げんご、language）は、狭義には「声による記号の体系」をいう。\\n広辞苑や大辞泉...\n",
      "2    11     日本語  日本語（にほんご、にっぽんご、）は、日本国内や、かつての日本領だった国、そして国外移民や移住...\n"
     ]
    }
   ],
   "source": [
    "def split_surface_to_sentences(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    DataFrame内の'surface'列に含まれるテキストを文に分割し、新しいDataFrameを作成する関数。\n",
    "    各行は'surface'列のテキストを句点「。」で分割し、それぞれの文を新しい行として追加する。\n",
    "    分割された文は'surface'列に格納される。\n",
    "    df: 分割する文が含まれるDataFrame。'surface'列が必要。\n",
    "    戻り値: 'surface'列に分割された文を含む新しいDataFrame。\n",
    "    \"\"\"\n",
    "    def split_sentences(row):\n",
    "        # 'surface'列のテキストを行ごとに分割し、さらに句点「。」で文に分割する\n",
    "        sentences = [sentence for line in row['surface'].splitlines() for sentence in line.split('。') if sentence]\n",
    "        # 分割された文を新しいDataFrameに格納し、他の列は元の値を繰り返す\n",
    "        return pd.DataFrame({col: [row[col]]*len(sentences) if col != 'surface' else sentences for col in df.columns})\n",
    "\n",
    "    # 全ての行に対してsplit_sentences関数を適用し、結果を結合する\n",
    "    new_df = pd.concat(df.apply(split_sentences, axis=1).tolist(), ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "subset = dataset[\"train\"][:3]\n",
    "article_df = pd.DataFrame(subset)\n",
    "article_df.rename(columns={\"text\": \"surface\"}, inplace=True)\n",
    "passage_df = split_surface_to_sentences(article_df)\n",
    "print(len(article_df))\n",
    "print(article_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1589it [00:01, 1329.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     curid title                                            surface  \\\n",
      "59      10    言語  言語と非言語の境界が問題になるが、文字を使う方法と文字を用いない方法の区別のみで、言語表現を...   \n",
      "106     10    言語      同一語族に属する言語群の場合、共通語彙から言語の分化した年代を割り出す方法も考案されている   \n",
      "194     10    言語  学校教育もこの言語で行われるが、民族語とかけ離れた存在であることもあり国民の中で使用できる層...   \n",
      "297     11   日本語  大野晋は日本語が語彙・文法などの点でタミル語と共通点を持つとの説を唱えるが、比較言語学の方法...   \n",
      "336     11   日本語         これは、「直前の母音を1モーラ分引く」という方法で発音される独立した特殊モーラである   \n",
      "345     11   日本語  たとえば、「です」「ます」は のように発音されるし、「菊」「力」「深い」「放つ」「秋」などは...   \n",
      "369     11   日本語  なお、「だ行」の「ぢ」「づ」は、一部方言を除いて「ざ行」の「じ」「ず」と同音に帰しており、発...   \n",
      "538     11   日本語                                       」「姉さん、どこへ行くの   \n",
      "578     11   日本語        英訳で \"We hold…\"（われらは信ずる）と主語・述語が隣り合うのとは対照的である   \n",
      "614     11   日本語  「気を引かれる」「私は泣かない」「花が笑った」「さあ、出かけよう」「今日は来ないそうだ」「も...   \n",
      "634     11   日本語  なお、上記はあくまでも典型的な機能であり、主体を表さない「が」（例、「水が飲みたい」）、対象...   \n",
      "718     11   日本語  漫画家の手塚治虫は、漫画を英訳してもらったところ、「ドギューン」「シーン」などの語に翻訳者が...   \n",
      "752     11   日本語                      この説明方法は、現在の学校教育の国語でも取り入れられている   \n",
      "808     11   日本語  たとえば、「弱」という造語成分は、「脆」「貧」「軟」「薄」などの成分と結合することにより、「...   \n",
      "810     11   日本語  現代語としても、「国立」「改札」「着席」「挙式」「即答」「熱演」など多くの和製漢語が用いられている   \n",
      "824     11   日本語  「語種」の節で触れた混種語、すなわち、「プロ野球」「草野球」「日本シリーズ」のように複数の語...   \n",
      "874     11   日本語         また、「垰」は「たお」「たわ」などと読まれる国字で、中国地方ほかで定着しているという   \n",
      "879     11   日本語  このことは、「ですます体」「でございます体」「だ体」「である体」「ありんす言葉」（江戸・新吉...   \n",
      "881     11   日本語  なお、「文体」の用語は、書かれた文章だけではなく談話についても適用されるため、以下では「文体...   \n",
      "893     11   日本語                           ここから、丁寧体を「ですます体」と呼ぶこともある   \n",
      "1000    11   日本語  一般にはアクセントの違いは日本語の東西の違いとして語られることが多いが、実際の分布は単純な東...   \n",
      "1005    11   日本語  これら有アクセントの方言に対し、東北地方南部から関東地方北東部にかけての地域や、九州の東京式...   \n",
      "1196    11   日本語                漢字を除き、他言語の語彙を借用することは、古代にはそれほど多くなかった   \n",
      "1280    11   日本語  当初漢文は中国語音で読まれたとみられるが、日本語と中国語の音韻体系は相違が大きいため、この方...   \n",
      "1319    11   日本語        『万葉集』の巻14「東歌」や巻20「防人歌」には当時の東国方言による歌が記録されている   \n",
      "1511    11   日本語  漢字や仮名と同じように日本語の文字として扱われ、約物のような利用方法にとどまらず、単語や文章...   \n",
      "\n",
      "                                         surface_tokens  \\\n",
      "59    [言語, と, 非, 言語, の, 境界, が, 問題, に, なる, が, 、, 文字, ...   \n",
      "106   [同一, 語, 族, に, 属する, 言語, 群, の, 場合, 、, 共通, 語彙, から...   \n",
      "194   [学校, 教育, も, この, 言語, で, 行わ, れる, が, 、, 民族, 語, と,...   \n",
      "297   [大野, 晋, は, 日本, 語, が, 語彙, ・, 文法, など, の, 点, で, タ...   \n",
      "336   [これ, は, 、, 「, 直前, の, 母音, を, 1, モーラ, 分, 引く, 」, ...   \n",
      "345   [たとえば, 、, 「, です, 」, 「, ます, 」, は, の, よう, に, 発音,...   \n",
      "369   [なお, 、, 「, だ行, 」, の, 「, ぢ, 」, 「, づ, 」, は, 、, 一...   \n",
      "538                      [」, 「, 姉, さん, 、, どこ, へ, 行く, の]   \n",
      "578   [英訳, で, \", We, hold, …, \"（, われ, ら, は, 信ずる, ）, ...   \n",
      "614   [「, 気, を, 引か, れる, 」, 「, 私, は, 泣か, ない, 」, 「, 花,...   \n",
      "634   [なお, 、, 上記, は, あくまで, も, 典型, 的, な, 機能, で, あり, 、...   \n",
      "718   [漫画, 家, の, 手塚, 治虫, は, 、, 漫画, を, 英訳, し, て, もらっ,...   \n",
      "752   [この, 説明, 方法, は, 、, 現在, の, 学校, 教育, の, 国語, で, も,...   \n",
      "808   [たとえば, 、, 「, 弱, 」, と, いう, 造語, 成分, は, 、, 「, 脆, ...   \n",
      "810   [現代, 語, と, し, て, も, 、, 「, 国立, 」, 「, 改札, 」, 「, ...   \n",
      "824   [「, 語種, 」, の, 節, で, 触れ, た, 混種, 語, 、, すなわち, 、, ...   \n",
      "874   [また, 、, 「, 垰, 」, は, 「, たお, 」, 「, た, わ, 」, など, ...   \n",
      "879   [この, こと, は, 、, 「, です, ます, 体, 」, 「, で, ござい, ます,...   \n",
      "881   [なお, 、, 「, 文体, 」, の, 用語, は, 、, 書か, れ, た, 文章, だ...   \n",
      "893   [ここ, から, 、, 丁寧, 体, を, 「, です, ます, 体, 」, と, 呼ぶ, ...   \n",
      "1000  [一般, に, は, アクセント, の, 違い, は, 日本, 語, の, 東西, の, 違...   \n",
      "1005  [これ, ら, 有, アクセント, の, 方言, に, 対し, 、, 東北, 地方, 南部,...   \n",
      "1196  [漢字, を, 除き, 、, 他, 言語, の, 語彙, を, 借用, する, こと, は,...   \n",
      "1280  [当初, 漢文, は, 中国, 語音, で, 読ま, れ, た, と, み, られる, が,...   \n",
      "1319  [『, 万葉, 集, 』, の, 巻, 14, 「, 東歌, 」, や, 巻, 20, 「,...   \n",
      "1511  [漢字, や, 仮名, と, 同じ, よう, に, 日本, 語, の, 文字, と, し, ...   \n",
      "\n",
      "                                   pronunciation_tokens  \\\n",
      "59    [ゲンゴ, ト, ヒ, ゲンゴ, ノ, キョーカイ, ガ, モンダイ, ニ, ナル, ガ, ...   \n",
      "106   [ドーイツ, ゴ, ゾク, ニ, ゾクスル, ゲンゴ, グン, ノ, バアイ, , キョーツ...   \n",
      "194   [ガッコー, キョーイク, モ, コノ, ゲンゴ, デ, オコナワ, レル, ガ, , ミン...   \n",
      "297   [オーノ, ススム, ワ, ニッポン, ゴ, ガ, ゴイ, , ブンポー, ナド, ノ, テ...   \n",
      "336   [コレ, ワ, , , チョクゼン, ノ, ボイン, オ, 1, モーラ, ブン, ヒク, ...   \n",
      "345   [タトエバ, , , デス, , , マス, , ワ, ノ, ヨー, ニ, ハツオン, サ,...   \n",
      "369   [ナオ, , , ダギョー, , ノ, , ジ, , , ズ, , ワ, , イチブ, ホー...   \n",
      "538                        [, , アネ, サン, , ドコ, エ, イク, ノ]   \n",
      "578   [エーヤク, デ, \", We, hold, , \"（, ワレ, ラ, ワ, シンズル, ,...   \n",
      "614   [, キ, オ, ヒカ, レル, , , ワタクシ, ワ, ナカ, ナイ, , , ハナ, ...   \n",
      "634   [ナオ, , ジョーキ, ワ, アクマデ, モ, テンケー, テキ, ナ, キノー, デ, ...   \n",
      "718   [マンガ, カ, ノ, テズカ, オサム, ワ, , マンガ, オ, エーヤク, シ, テ,...   \n",
      "752   [コノ, セツメー, ホーホー, ワ, , ゲンザイ, ノ, ガッコー, キョーイク, ノ,...   \n",
      "808   [タトエバ, , , ジャク, , ト, ユー, ゾーゴ, セーブン, ワ, , , モロ,...   \n",
      "810   [ゲンダイ, ゴ, ト, シ, テ, モ, , , コクリツ, , , カイサツ, , , ...   \n",
      "824   [, ゴシュ, , ノ, フシ, デ, フレ, タ, コンシュ, ゴ, , スナワチ, , ...   \n",
      "874   [マタ, , , タオ, , ワ, , タオ, , , タ, ワ, , ナド, ト, ヨマ,...   \n",
      "879   [コノ, コト, ワ, , , デス, マス, カラダ, , , デ, ゴザイ, マス, カ...   \n",
      "881   [ナオ, , , ブンタイ, , ノ, ヨーゴ, ワ, , カカ, レ, タ, ブンショー,...   \n",
      "893   [ココ, カラ, , テーネー, タイ, オ, , デス, マス, カラダ, , ト, ヨブ...   \n",
      "1000  [イッパン, ニ, ワ, アクセント, ノ, チガイ, ワ, ニッポン, ゴ, ノ, トーザ...   \n",
      "1005  [コレ, ラ, ユー, アクセント, ノ, ホーゲン, ニ, タイシ, , トーホク, チホ...   \n",
      "1196  [カンジ, オ, ノゾキ, , タ, ゲンゴ, ノ, ゴイ, オ, シャクヨー, スル, コ...   \n",
      "1280  [トーショ, カンブン, ワ, チューゴク, ゴオン, デ, ヨマ, レ, タ, ト, ミ,...   \n",
      "1319  [, マンヨー, シュー, , ノ, マキ, 14, , アズマウタ, , ヤ, マキ, 2...   \n",
      "1511  [カンジ, ヤ, カナ, ト, オナジ, ヨー, ニ, ニッポン, ゴ, ノ, モジ, ト,...   \n",
      "\n",
      "                                          pronunciation  \\\n",
      "59    ゲンゴトヒゲンゴノキョーカイガモンダイニナルガモジオツカウホーホートモジオモチーナイホーホー...   \n",
      "106   ドーイツゴゾクニゾクスルゲンゴグンノバアイキョーツーゴイカラゲンゴノブンカシタネンダイオワリ...   \n",
      "194   ガッコーキョーイクモコノゲンゴデオコナワレルガミンゾクゴトカケハナレタソンザイデアルコトモア...   \n",
      "297   オーノススムワニッポンゴガゴイブンポーナドノテンデタミルゴトキョーツーテンオモツトノセツオト...   \n",
      "336   コレワチョクゼンノボインオ1モーラブンヒクトユーホーホーデハツオンサレルドクリツシタトクシュ...   \n",
      "345   タトエバデスマスワノヨーニハツオンサレルシキクチカラフカイハナツアキナドワソレゾレトハツオン...   \n",
      "369   ナオダギョーノジズワイチブホーゲンオノゾイテザギョーノジズトドーオンニカエシテオリハツオンホ...   \n",
      "538                                          アネサンドコエイクノ   \n",
      "578   エーヤクデ\"Wehold\"（ワレラワシンズルトシュゴジュツゴガトナリアウノトワタイショーテキデアル   \n",
      "614   キオヒカレルワタクシワナカナイハナガワラッタサーデカケヨーキョーワコナイソーダモースグハルデ...   \n",
      "634   ナオジョーキワアクマデモテンケーテキナキノーデアリシュタイオアラワサナイガレーミズガノミタイ...   \n",
      "718   マンガカノテズカオサムワマンガオエーヤクシテモラッタトコロドギューンシーンナドノゴニホンヤク...   \n",
      "752            コノセツメーホーホーワゲンザイノガッコーキョーイクノコクゴデモトリイレラレテイル   \n",
      "808   タトエバジャクトユーゾーゴセーブンワモロヒンナンウスナドノセーブントケツゴースルコトニヨリゼ...   \n",
      "810   ゲンダイゴトシテモコクリツカイサツチャクセキキョシキソクトーネツエンナドオークノワセーカンゴ...   \n",
      "824   ゴシュノフシデフレタコンシュゴスナワチプロヤキュークサヤキューニッポンシリーズノヨーニフクス...   \n",
      "874         マタタオワタオタワナドトヨマレルコクジデチューゴクチホーホカデテーチャクシテイルトイウ   \n",
      "879   コノコトワデスマスカラダデゴザイマスカラダダタイデアルカラダアリンスコトバエドシンヨシワラノ...   \n",
      "881   ナオブンタイノヨーゴワカカレタブンショーダケデワナクダンワニツイテモテキヨーサレルタメイカデ...   \n",
      "893                          ココカラテーネータイオデスマスカラダトヨブコトモアル   \n",
      "1000  イッパンニワアクセントノチガイワニッポンゴノトーザイノチガイトシテカタラレルコトガオーイガジ...   \n",
      "1005  コレラユーアクセントノホーゲンニタイシトーホクチホーナンブカラカントーチホーホクトーブニカケ...   \n",
      "1196          カンジオノゾキタゲンゴノゴイオシャクヨースルコトワコダイニワソレホドオークナカッタ   \n",
      "1280  トーショカンブンワチューゴクゴオンデヨマレタトミラレルガニッポンゴトチューゴクゴノオンインタ...   \n",
      "1319  マンヨーシューノマキ14アズマウタヤマキ20サキモリウタニワトージノトーゴクホーゲンニヨルウ...   \n",
      "1511  カンジヤカナトオナジヨーニニッポンゴノモジトシテアツカワレヤクモノノヨーナリヨーホーホーニト...   \n",
      "\n",
      "                                                hitword  \n",
      "59    {'hitwords': [{'hit': True, 'surface': 'ホーホー',...  \n",
      "106   {'hitwords': [{'hit': True, 'surface': 'ホーホー',...  \n",
      "194   {'hitwords': [{'hit': True, 'surface': 'ドオー', ...  \n",
      "297   {'hitwords': [{'hit': True, 'surface': 'ホーホー',...  \n",
      "336   {'hitwords': [{'hit': True, 'surface': 'ホーホー',...  \n",
      "345   {'hitwords': [{'hit': True, 'surface': 'デスマス',...  \n",
      "369   {'hitwords': [{'hit': True, 'surface': 'ホーホー',...  \n",
      "538   {'hitwords': [{'hit': True, 'surface': 'サンド', ...  \n",
      "578   {'hitwords': [{'hit': True, 'surface': 'ヤクデ', ...  \n",
      "614   {'hitwords': [{'hit': True, 'surface': 'ラッタ', ...  \n",
      "634   {'hitwords': [{'hit': True, 'surface': 'ドードー',...  \n",
      "718   {'hitwords': [{'hit': True, 'surface': 'ラッタ', ...  \n",
      "752   {'hitwords': [{'hit': True, 'surface': 'ホーホー',...  \n",
      "808   {'hitwords': [{'hit': True, 'surface': 'ゴース', ...  \n",
      "810   {'hitwords': [{'hit': True, 'surface': 'ドオー', ...  \n",
      "824   {'hitwords': [{'hit': True, 'surface': 'フシデ', ...  \n",
      "874   {'hitwords': [{'hit': True, 'surface': 'ホーホー',...  \n",
      "879   {'hitwords': [{'hit': True, 'surface': 'デスマス',...  \n",
      "881   {'hitwords': [{'hit': True, 'surface': 'メテノ', ...  \n",
      "893   {'hitwords': [{'hit': True, 'surface': 'デスマス',...  \n",
      "1000  {'hitwords': [{'hit': True, 'surface': 'ホーホー',...  \n",
      "1005  {'hitwords': [{'hit': True, 'surface': 'ホーホー',...  \n",
      "1196  {'hitwords': [{'hit': True, 'surface': 'ドオー', ...  \n",
      "1280  {'hitwords': [{'hit': True, 'surface': 'ホーホー',...  \n",
      "1319  {'hitwords': [{'hit': True, 'surface': 'キモリ', ...  \n",
      "1511  {'hitwords': [{'hit': True, 'surface': 'ホーホー',...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [00:00, 25366.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   curid title word_surface word_pronunciation  word_length  \\\n",
      "0     10    言語         ホーホー                ホーホ            3   \n",
      "2     10    言語          ドオー                ドオー            3   \n",
      "5     11   日本語         デスマス               デスマス            4   \n",
      "7     11   日本語          サンド                サンド            3   \n",
      "8     11   日本語          ヤクデ                ヤクデ            3   \n",
      "9     11   日本語          ラッタ                ラッタ            3   \n",
      "10    11   日本語         ドードー               ドードー            4   \n",
      "11    11   日本語          ラッタ                ラッタ            3   \n",
      "13    11   日本語          ゴース                ゴース            3   \n",
      "14    11   日本語          ドオー                ドオー            3   \n",
      "15    11   日本語          フシデ                フシデ            3   \n",
      "16    11   日本語         ホーホー                ホーホ            3   \n",
      "17    11   日本語         デスマス               デスマス            4   \n",
      "18    11   日本語          メテノ                メテノ            3   \n",
      "20    11   日本語         ホーホー                ホーホ            3   \n",
      "21    11   日本語         ホーホー                ホーホ            3   \n",
      "22    11   日本語          ドオー                ドオー            3   \n",
      "24    11   日本語          キモリ                キモリ            3   \n",
      "\n",
      "    is_crossing_word_boundary                                     context  \\\n",
      "0                       False           境界が問題になるが、文字を使う方法と文字を用いない方法の区別のみで   \n",
      "2                       False                      もあり国民の中で使用できる層はさほど多くない   \n",
      "5                       False                   たとえば、「です」「ます」はのように発音されるし、   \n",
      "7                       False                                」「姉さん、どこへ行くの   \n",
      "8                       False                       英訳で\"Wehold…\"（われらは信ずる）   \n",
      "9                       False             」「私は泣かない」「花が笑った」「さあ、出かけよう」「今日は来   \n",
      "10                      False              点を表さない「に」（例、受動動作の主体「先生にほめられた」、   \n",
      "11                      False       の手塚治虫は、漫画を英訳してもらったところ、「ドギューン」「シーン」などの   \n",
      "13                      False              「軟」「薄」などの成分と結合することにより、「脆弱」「貧弱」   \n",
      "14                      False              」「挙式」「即答」「熱演」など多くの和製漢語が用いられている   \n",
      "15                      False                    「語種」の節で触れた混種語、すなわち、「プロ野球   \n",
      "16                       True               わ」などと読まれる国字で、中国地方ほかで定着しているという   \n",
      "17                      False                    このことは、「ですます体」「でございます体」「だ   \n",
      "18                      False                           は「文体」に「話体」も含めて述べる   \n",
      "20                       True  なく、東京式アクセントは概ね北海道、東北地方北部、関東地方西部、甲信越地方、東海地方   \n",
      "21                       True          の方言に対し、東北地方南部から関東地方北東部にかけての地域や、九州の   \n",
      "22                      False                     を借用することは、古代にはそれほど多くなかった   \n",
      "24                      False               の巻14「東歌」や巻20「防人歌」には当時の東国方言による   \n",
      "\n",
      "   matched_passage_surface  sum_length  max_length  num  \n",
      "0                       方法           3           3    1  \n",
      "2                    さほど多く           3           3    1  \n",
      "5                   です」「ます           4           4    1  \n",
      "7                    さん、どこ           3           3    1  \n",
      "8                      英訳で           3           3    1  \n",
      "9                      笑った           3           3    1  \n",
      "10                    受動動作           4           4    1  \n",
      "11                    もらった           3           3    1  \n",
      "13                    結合する           3           3    1  \n",
      "14                    など多く           3           3    1  \n",
      "15                      節で           3           3    1  \n",
      "16                    地方ほか           3           3    1  \n",
      "17                    ですます           4           4    1  \n",
      "18                  含めて述べる           3           3    1  \n",
      "20                    地方北部           3           3    1  \n",
      "21                    地方北東           3           3    1  \n",
      "22                    ほど多く           3           3    1  \n",
      "24                      防人           3           3    1  \n"
     ]
    }
   ],
   "source": [
    "detector = SerendipitousWordDetector()\n",
    "word_df = pd.read_csv(WORDLIST_PATH)\n",
    "# check_hitsメソッドをテスト\n",
    "hitwords_df = detector.check_hits(df, passage_df)\n",
    "\n",
    "converted_df = detector.convert_table(hitwords_df)\n",
    "\n",
    "hitwords_df.to_csv(f\"data/poke_hitwords_{len(article_df)}.csv\", index=False)\n",
    "converted_df.to_csv(f\"data/poke_hitwords_{len(article_df)}_converted.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "12d149281988f5406b2a3f64efb612a49acfd85bb46b4cf7b35557663c001ac0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
